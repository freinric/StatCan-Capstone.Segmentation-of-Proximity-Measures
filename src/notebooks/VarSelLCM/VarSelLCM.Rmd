---
title: "VarSelLCM"
author: "Avishek"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache=TRUE, 
                      fig.align="center", 
                      fig.width=8, 
                      fig.height=6)

library(VarSelLCM)
```

* * *

# Preliminary 

## Loading \& Cleaning Data

```{r}
set.seed(2023)
library(dplyr)
# library(cluster)
# library(ClustImpute)
# library(ggplot2)
# library(factoextra)
# library(clusterCrit)
load('../../../data/master_pms_df.Rdata')

# Convert selected columns to numeric
master <- master %>%
  mutate(PMS_DBPOP = as.numeric(gsub(",", "", PMS_DBPOP)), # Dissemination block population
         PMS_DAPOP = as.numeric(gsub(",", "", PMS_DAPOP)), # Dissemination area population
         PMS_CSDPOP = as.numeric(gsub(",", "", PMS_CSDPOP)), # Census subdivision population
         PMS_CMAPOP = as.numeric(gsub(",", "", PMS_CMAPOP)), # Census metropolitan area population
         PMS_PRPOP = as.numeric(gsub(",", "", PMS_PRPOP)), # Province or territory population
         # in_db_emp = as.numeric(in_db_emp),
         # in_db_pharma = as.numeric(in_db_pharma),
         # in_db_childcare = as.numeric(in_db_childcare),
         # in_db_health = as.numeric(in_db_health),
         # in_db_grocery = as.numeric(in_db_grocery),
         # in_db_educpri = as.numeric(in_db_educpri),
         # in_db_educsec = as.numeric(in_db_educsec),
         # in_db_lib = as.numeric(in_db_lib),
         # in_db_parks = as.numeric(in_db_parks),
         # in_db_transit = as.numeric(in_db_transit),
         PMS_prox_idx_emp = as.numeric(PMS_prox_idx_emp),
         PMS_prox_idx_pharma = as.numeric(PMS_prox_idx_pharma),
         PMS_prox_idx_childcare = as.numeric(PMS_prox_idx_childcare),
         PMS_prox_idx_health = as.numeric(PMS_prox_idx_health),
         PMS_prox_idx_grocery = as.numeric(PMS_prox_idx_grocery),
         PMS_prox_idx_educpri = as.numeric(PMS_prox_idx_educpri),
         PMS_prox_idx_educsec = as.numeric(PMS_prox_idx_educsec),
         PMS_prox_idx_lib = as.numeric(PMS_prox_idx_lib),
         PMS_prox_idx_parks = as.numeric(PMS_prox_idx_parks),
         PMS_prox_idx_transit = as.numeric(PMS_prox_idx_transit),
         DBUID = as.character(DBUID),
         PMS_DAUID = as.character(PMS_DAUID),
         PMS_CSDUID = as.character(PMS_CSDUID),
         PMS_CMAUID = as.character(PMS_CMAUID),
         PMS_CMAPUID = as.character(PMS_CMAPUID),
         PMS_PRUID = as.character(PMS_PRUID),
         PMS_suppressed = as.character(PMS_suppressed),
         PMS_transit_na = as.character(PMS_suppressed))

# subsampling data 
perc = 3 #percentage of data to subsample
subsample = (nrow(master)/100)*perc 
master_sample = master[sample(nrow(master), subsample),]
master_sample = na.omit(master_sample)
```


```{r}
# variables to cluster with
clust_vars = c('CSD_AREA', 'PMS_CSDPOP', 'PMS_DBPOP', 'IOR_Index_of_remoteness') #, 'PMS_CMATYPE') --> clustimpute can only cluster over numeric variables!
```

```{r}
# Subset columns that start with "prox_idx"
amenities <- colnames(master)[grepl("^PMS_prox_idx", colnames(master))]
master_amenities <- master[, amenities]
master_amenities_wo_na <- na.omit(master_amenities)
```

```{r eval=FALSE}
plot(master_amenities)
```

```{r}
# cutoff values for all amenities
all_cutoffs = list()

#cutoff function
#https://stats.stackexchange.com/questions/586937/identify-the-point-of-intersection-from-two-distributions
cutoff = function(x, y){
  # Find global minimum and maximum
  xymin <- min(x,y)
  xymax <- max(x,y)
# Estimate densities
  dx <- density(x, n=512, from=xymin, to=xymax)
  dy <- density(y, n=512, from=xymin, to=xymax)

# Plot results
  #plot(dx, xlim=c(xymin, xymax), type="l", lwd=3, xlab="X", ylab="Density", main="")
  #lines(dy, col="red", lwd=3)

# Differences in densities
  dx$diff <- dx$y - dy$y
  ex <- NULL  # Store the intersection points
  ey <- NULL
  k = 0
  for (i in 2:length(dx$x)) {
      # Look for a change in sign of the difference in densities
      if (sign(dx$diff[i-1]) != sign(dx$diff[i])) {
         k = k + 1
         # Linearly interpolate
         ex[k] <- dx$x[i-1] + (dx$x[i]-dx$x[i-1])*(0-dx$diff[i-1])/(dx$diff[i]-dx$diff[i-1])
         ey[k] <- dx$y[i-1] + (dx$y[i]-dx$y[i-1])*(ex[k]-dx$x[i-1])/(dx$x[i]-dx$x[i-1])
         #lines(c(ex[k],ex[k]), c(0,ey[k])) #more plotting
         #points(ex[k], ey[k], pch=16, col="green" )
    }
  }
  exf = ex[ey > 0.5 & ex > 0.01]
  eyf = ey[ey > 0.5 & ex > 0.01]

  if (length(exf) > 1){
    #stop('More than one intersection above 0.5 detected! Change y-axis threshold.')
    exf = exf[1]
    eyf = eyf[1]
  }

  return(c(exf, eyf))
}

#mode function
Mode = function(x){
  ux = unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

#print list items function for profiles
print_list = function(x, cl){
  df = as.data.frame(x)
  if (ncol(df) == 1){
    df = as.data.frame(t(df))
    row.names(df) = ''
  }
  names(df) = paste('Cluster', cl)
  print(df)
  cat('\n\n')
}
```


# Clustering Tendency

There are various ways to assess the clustering tendency of the data. Hopkins Test is used to test the probability that the data is generated by a uniform random distribution and is often used as a proxy measure for clustering tendency.

It works by selecting a sample from the data, the size of which is specified by the analyst. For each point find its distance to its nearest neighbour. Then the algorithm generates a simulated data set from a uniform random distribution and finds the distance from points in this data set from their nearest neighbours. To calculate the Hopkins statistic you divide the average nearest neighbour distance in the random dataset by the average nearest neighbour distance in the random dataset plus the average nearest neighbour distance in the actual dataset. Values close to 0.5 indicate high degree of spatial randomness.

```{r}
library(hopkins)
hopkins <- hopkins(master_amenities_wo_na)
hopkins
```

The result of the Hopkins Test is 1 which is not well. If it's below the 0.5 threshold and therefore indicates the data is clusterable. In this case its not clusterable using all the proximity measures.

```{r}
temp <- data.frame(PMS_prox_idx_emp = master_amenities$PMS_prox_idx_emp, PMS_CSDPOP = master$PMS_CSDPOP)
```

```{r}
temp_wo_na <- na.omit(temp)
hopkins <- hopkins(temp_wo_na)
hopkins
```
```{r}
sum(is.na(temp_wo_na))
```

```{r}
test_df <- master[, c("IOR_Index_of_remoteness", "PMS_prox_idx_emp")]
```

```{r}
# subsampling data 
perc = 3 #percentage of data to subsample
subsample = (nrow(test_df)/100)*perc 
test_df = test_df[sample(nrow(test_df), subsample),]
test_df = na.omit(test_df)
```


```{r}
cluster_df <- VarSelCluster(test_df, gvals=4, nbcores = 4, vbleSelec = FALSE)
```

```{r}
# the structure of s4 object 
# str(cluster_df@data)
```


```{r}
# Load the factoextra package
library(factoextra)
library(clusterCrit)

# Obtain the cluster assignments from VarSelCluster
cluster_assignments <- cluster_df@partitions@zMAP

# Obtain the data from VarselCluster
cluster_data <- cluster_df@data@dataContinuous@data

# Calculate the silhouette scores
silhouette_scores <- intCriteria(as.matrix(cluster_data), as.integer(cluster_assignments), 'Silhouette') 

```


```{r}
# silhouette_scores_list <- c()
silhouette_scores_list[4] <- c(silhouette_scores)
```








## Assumptions of the Alogrithm

Algorithm steps:






```{r}
num = 2:6
for (i in num){
  print(i)
}
```

```{r}
library(cluster)
```




```{r}
#algorithm function --> must return: data with no NAs, cluster assignments
algo = function(dataset, amen_name, num=NULL){
  sil_coefs = c()
  counter = 1
  for (i in num){
    nr_cluster = i # number of clusters
    res = VarSelCluster(dataset, gvals=nr_cluster, nbcores = 4, vbleSelec = FALSE)
    
    # Obtain the cluster assignments from VarSelCluster
    cluster_assignments <- res@partitions@zMAP
    
    # Obtain the data from VarselCluster
    cluster_data <- res@data@dataContinuous@data
    
    sil_coefs[counter] = intCriteria(as.matrix(cluster_data), 
                                     as.integer(cluster_assignments), 
                                     'Silhouette')$silhouette
    counter = counter + 1
  }
  
  #plot silhouette coefficients
  plot(sil_coefs~num, type = 'l', ylab="Silhouette Coefficient", xlab='Number of Clusters', lwd=2)
  
  #re-run algorithm with highest sil
  res = VarSelCluster(dataset, gvals=num[which(sil_coefs == max(sil_coefs))], nbcores = 4, vbleSelec = FALSE) 
  
  return(list('complete_data' = res@data@dataContinuous@data, 'clusts' = res@partitions@zMAP))
}

#master function
do_everything = function(dataset, amen_name, num){
  # remove NA values
  amen = dataset[!is.na(dataset[,amen_name]),]
  clust_data = dataset[!is.na(dataset[,amen_name]),c(amen_name, clust_vars)]
  
  # algorithm
  res = algo(clust_data, amen_name, num)
  
  # store cluster results
  clusts = res$clusts
  comp_data = res$complete_data
  rm(res)
  
  # re-assign clusters so that they're in order
  comp = list()
  for (i in unique(clusts)){
    temp = amen[clusts == i,amen_name]
    comp[[i]] = Mode(round(temp, 6))
  }
  if (max(unlist(comp)) == comp[[1]]){
    ord = match(comp, comp[order(unlist(comp))])
  } else{
    ord = match(comp[order(unlist(comp))], comp) 
  }
  clusts = ord[clusts]
  
  # plot
  pass = list(data = comp_data, cluster = clusts)
  plot(fviz_cluster(pass, ellipse.type = "norm") + theme_minimal())
  
  cutoffs = list()
  all_cutoffs = list()
  plot(density(amen[clusts == 1,amen_name]), xlim=c(0, 1), lwd=2, xlab="X", ylab="Density", main="")
  for (j in 1:(length(unique(clusts))-1)){
      cutoffs[[j]] = cutoff(amen[clusts == j,amen_name], amen[clusts == j+1,amen_name])
      all_cutoffs[[amen_name]][j] = cutoffs[[j]][1]
      lines(density(amen[clusts == j+1,amen_name]), col=(j+1), lwd=2)
      points(cutoffs[[j]][1], cutoffs[[j]][2], pch=16, col="green" )
  }

  print('Segment cutoff values:')
  for (a in 1:length(cutoffs)){
    print(cutoffs[[a]][1])
  }
  
  #silhouette plot
  sil = silhouette(clusts, dist(comp_data))
  plot(fviz_silhouette(sil))
  
  #profiles
  print('Cluster profiles:')
  profiles = list()
  for (k in sort(unique(clusts))){
    temp = dataset[clusts == k,]
    profiles[['Count']][k] = as.character(nrow(temp))
    profiles[['DB_population']][k] = round(mean(temp$PMS_DBPOP, na.rm = T), 1)
    profiles[['CSD_population']][k] = round(mean(temp$PMS_CSDPOP, na.rm = T), 1)
    profiles[['CMA_type']][[k]] = summary(temp$PMS_CMATYPE)
    profiles[['Index_of_Remoteness']][k] = round(mean(temp$IOR_Index_of_remoteness, na.rm = T), 3)
    profiles[['Provinces']][[k]] = summary(temp$PROVINCE)
    profiles[['Amenity_dense']][[k]] = summary(temp$PMS_amenity_dense)
  }
  #printing profiles
  print('Num of DBs:')
  print_list(profiles[['Count']], sort(unique(clusts)))
  cat('\n DB Population: \n')
  print_list(profiles[['DB_population']], sort(unique(clusts)))
  cat('\n CSD Population: \n')
  print_list(profiles[['CSD_population']], sort(unique(clusts)))
  cat('\n CMA Type: \n')
  print_list(profiles[['CMA_type']], sort(unique(clusts)))
  cat('\n Index of Remoteness: \n')
  print_list(profiles[['Index_of_Remoteness']], sort(unique(clusts)))
  cat('\n Provinces: \n')
  print_list(profiles[['Provinces']], sort(unique(clusts)))
  cat('\n Amenity dense: \n')
  print_list(profiles[['Amenity_dense']], sort(unique(clusts)))
  
  #return all_cutoff values
  return(all_cutoffs)
}
```




## Employment

```{r}
lst = do_everything(master_sample, 'PMS_prox_idx_emp', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```














