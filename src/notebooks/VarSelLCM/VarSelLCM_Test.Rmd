---
title: "VarSelLCM_Test"
author: "Avishek"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache = TRUE, 
                      # warning = F,  
                      fig.align = "center", 
                      fig.width = 5, 
                      fig.height = 5)
```

```{r}
set.seed(2023)
library(dplyr)
library(cluster)
library(ggplot2)
library(factoextra)
library(clusterCrit)
library(VarSelLCM)
library(clustertend)
load('../../../data/master_pms_df.Rdata')

# Convert selected columns to numeric
master <- master %>%
  mutate(PMS_DBPOP = as.numeric(gsub(",", "", PMS_DBPOP)), # Dissemination block population
         PMS_DAPOP = as.numeric(gsub(",", "", PMS_DAPOP)), # Dissemination area population
         PMS_CSDPOP = as.numeric(gsub(",", "", PMS_CSDPOP)), # Census subdivision population
         PMS_CMAPOP = as.numeric(gsub(",", "", PMS_CMAPOP)), # Census metropolitan area population
         PMS_PRPOP = as.numeric(gsub(",", "", PMS_PRPOP)), # Province or territory population
         # in_db_emp = as.numeric(in_db_emp),
         # in_db_pharma = as.numeric(in_db_pharma),
         # in_db_childcare = as.numeric(in_db_childcare),
         # in_db_health = as.numeric(in_db_health),
         # in_db_grocery = as.numeric(in_db_grocery),
         # in_db_educpri = as.numeric(in_db_educpri),
         # in_db_educsec = as.numeric(in_db_educsec),
         # in_db_lib = as.numeric(in_db_lib),
         # in_db_parks = as.numeric(in_db_parks),
         # in_db_transit = as.numeric(in_db_transit),
         PMS_prox_idx_emp = as.numeric(PMS_prox_idx_emp),
         PMS_prox_idx_pharma = as.numeric(PMS_prox_idx_pharma),
         PMS_prox_idx_childcare = as.numeric(PMS_prox_idx_childcare),
         PMS_prox_idx_health = as.numeric(PMS_prox_idx_health),
         PMS_prox_idx_grocery = as.numeric(PMS_prox_idx_grocery),
         PMS_prox_idx_educpri = as.numeric(PMS_prox_idx_educpri),
         PMS_prox_idx_educsec = as.numeric(PMS_prox_idx_educsec),
         PMS_prox_idx_lib = as.numeric(PMS_prox_idx_lib),
         PMS_prox_idx_parks = as.numeric(PMS_prox_idx_parks),
         PMS_prox_idx_transit = as.numeric(PMS_prox_idx_transit),
         DBUID = as.character(DBUID),
         PMS_DAUID = as.character(PMS_DAUID),
         PMS_CSDUID = as.character(PMS_CSDUID),
         PMS_CMAUID = as.character(PMS_CMAUID),
         PMS_CMAPUID = as.character(PMS_CMAPUID),
         PMS_PRUID = as.character(PMS_PRUID),
         PMS_suppressed = as.character(PMS_suppressed),
         PMS_transit_na = as.character(PMS_suppressed))


# Subset columns that start with "prox_idx"
amenities <- colnames(master)[grepl("^PMS_prox_idx", colnames(master))]
# master dataset - contains only proximity columns
master_amenities <- master[, amenities]

# # master dataset w/o NA - contains only proximity columns
# master_amenities_wo_na <- na.omit(master_amenities)

# variables to cluster with
# clust_vars = c('CSD_AREA', 'PMS_CSDPOP', 'PMS_DBPOP', 'IOR_Index_of_remoteness') #, 'PMS_CMATYPE')


# master_amenities_log <- master_amenities
# 
# # log transform on PMs in master amenities
# for (col in amenities){
#   master_amenities_log[, col] = log(master_amenities[, col])
# }

# subsampling data 
perc = 0.4 #percentage of data to subsample
subsample = (nrow(master)/100)*perc 
master_sample = master[sample(nrow(master), subsample),]
```

## Clustering Tendency

There are various ways to assess the clustering tendency of the data. Hopkins Test is used to test the probability that the data is generated by a uniform random distribution and is often used as a proxy measure for clustering tendency.

It works by selecting a sample from the data, the size of which is specified by the analyst. For each point find its distance to its nearest neighbour. Then the algorithm generates a simulated data set from a uniform random distribution and finds the distance from points in this data set from their nearest neighbours. To calculate the Hopkins statistic you divide the average nearest neighbour distance in the random dataset by the average nearest neighbour distance in the random dataset plus the average nearest neighbour distance in the actual dataset. Values close to 0.5 indicate high degree of spatial randomness.

The result of the Hopkins Test is 1 which is not good. If it's below the 0.5 threshold and therefore indicates the data is clusterable. In this case its not clusterable using all the proximity measures.

```{r eval=FALSE}
# still now -> hopkins doesnt work on single column
library(hopkins)

for (col in amenities) {
  set.seed(2023)
  hopkins <- hopkins(as.matrix(master_sample_wo_na[, col]))
  cat("Hopkins statistic for ", col,  hopkins, "\n")
}

```

```{r eval=FALSE}
for (col in amenities) {
  set.seed(2023)
  hopkins <- hopkins(as.matrix(master_sample_log_wo_na[, col]))
  cat("Hopkins statistic for ", col,  hopkins, "\n")
}
```



```{r eval=FALSE}
# hopkins test using factoextra package
get_clust_tendency(as.matrix(master_sample_log_wo_na[, "PMS_prox_idx_emp"]), n = nrow(master_sample_log_wo_na)-1,
                          graph = TRUE)
```


### VAT on PMS_prox_idx_emp original

```{r}
# visual assessment of cluster tendency (VAT)
fviz_dist(dist(as.matrix(master_sample_wo_na[, "PMS_prox_idx_emp"])), 
          show_labels = FALSE)+
  labs(title = "PMS EMP")
```

The data seems random.

### VAT on PMS_prox_idx_emp log transformed

```{r}
# visual assessment of cluster tendency (VAT)
fviz_dist(dist(as.matrix(master_sample_log_wo_na[, "PMS_prox_idx_emp"])), 
          show_labels = FALSE)+
  labs(title = "PMS EMP")
```

We can see that log transformed proximity index of employment is clusterable.

## Apply Clustering Technique
<!--
```{r}
library(NbClust)
nb <- NbClust(master_sample_log_wo_na_emp, distance = "euclidean",
                  min.nc = 2, max.nc = 9, 
                  method = "complete", index ="all")

# factoextra::fviz_nbclust(res.nbclust) + theme_minimal() + ggtitle("NbClust's optimal number of clusters")

```

```{r}
library("factoextra")
fviz_nbclust(nb$Best.nc)
```
```{r}
factoextra::fviz_nbclust(nb) + theme_minimal() + ggtitle("NbClust's optimal number of clusters")
```
-->

```{r}
# Mode function
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}


#algorithm function --> must return: data with no NAs, cluster assignments
algo = function(dataset, num=NULL){
  sil_coefs = c()
  xie_coefs = c()
  dunn_coefs = c()
  counter = 1
  for (i in num){
    nr_cluster = i # number of clusters
    res = VarSelCluster(dataset, gvals=nr_cluster, nbcores = 4, vbleSelec = FALSE)
    
    # Obtain the cluster assignments from VarSelCluster
    cluster_assignments <- res@partitions@zMAP
    
    # Obtain the data from VarselCluster
    cluster_data <- res@data@dataContinuous@data
    
    sil_coefs[counter] = intCriteria(as.matrix(cluster_data), 
                                     as.integer(cluster_assignments), 
                                     'Silhouette')$silhouette
    
    xie_coefs[counter] = intCriteria(as.matrix(cluster_data), 
                                     as.integer(cluster_assignments), 
                                     'Xie_Beni')$xie_beni
    
    dunn_coefs[counter] = intCriteria(as.matrix(cluster_data), 
                                     as.integer(cluster_assignments), 
                                     'Dunn')$dunn
    
    counter = counter + 1
  }
  
  # plot silhouette coefficients
  plot(sil_coefs~num, type = 'l', ylab="Silhouette Coefficient", xlab='Number of Clusters', lwd=2)
  
  # plot Xie_Beni coefficients
  plot(xie_coefs~num, type = 'l', ylab="Xie_Beni Coefficient", xlab='Number of Clusters', lwd=2)
  
  # plot Dunn Index coefficients
  plot(dunn_coefs~num, type = 'l', ylab="Dunn Coefficient", xlab='Number of Clusters', lwd=2)
  
  res_sil_coefs <- num[which(sil_coefs == max(sil_coefs))]
  res_xie_coefs <- num[which(xie_coefs == min(xie_coefs))]
  res_dunn_coefs <- num[which(dunn_coefs == max(dunn_coefs))]
  
  # Find the most suggested number of clusters
  most_frequent <- Mode(c(res_sil_coefs, res_xie_coefs, res_dunn_coefs))
  
  # re-run algorithm with highest sil
  res = VarSelCluster(dataset, gvals=most_frequent, nbcores = 4, vbleSelec = FALSE)
  
  return(list('Silhouette' = res_sil_coefs, 'Xie_Beni' = res_xie_coefs, 'Dunn' = res_dunn_coefs, 'Clust_res' = res))
  # return(list('complete_data' = res@data@dataContinuous@data, 'clusts' = res@partitions@zMAP))
}
```


```{r}
do_everything = function(dataset, amen_name, num){
  
  # remove NA values
  clust_data = as.matrix(na.omit(dataset[, c(amen_name)]))
  
  # log data
  clust_data_log = log(clust_data)
  
  # algorithm
  res = algo(clust_data, num)
  # res_log = algo(clust_data_log, num)
  
  # store cluster results
  clusts = res$Clust_res@partitions@zMAP
  comp_data = res$Clust_res@data@dataContinuous@data
  
  # clusts_log = res_log$Clust_res@partitions@zMAP
  # comp_data_log = res_log$Clust_res@data@dataContinuous@data
  
  # silhouette plot
  sil = silhouette(clusts, dist(comp_data))
  plot(fviz_silhouette(sil))
  
  # sil_log = silhouette(clusts_log, dist(comp_data_log))
  # plot(fviz_silhouette(sil_log))
  
  #return sil values
  return(sil)
}
```


Silhouette coefficients  
- Observations with a large silhouette coefficients (almost 1) are very well clustered.
- A small silhouette coefficients (around 0) means that the observation lies between two clusters.
- Observations with a negative silhouette coefficients are probably placed in the wrong cluster.

Xie_Beni coefficients
- The optimal c is the one with the smallest XB coefficient value

Dunn Index  
- The Dunn index has a value between zero and infinity and should be maximized. The Dunn score with high value are more desirable; which suggesting itâ€™s a good cluster.

```{r eval=FALSE}
# for (col in amenities) {
#   col_name <- gsub("^.*_(\\w+)$", "\\1", col)
#   assign(paste0("master_sample_log_wo_na_", col_name), as.matrix(master_sample_log_wo_na[, col]))
# }
```

### Employment 

```{r}
res_algo_emp = do_everything(master_sample, 'PMS_prox_idx_emp', 2:6)
```

### Health 

```{r}
res_algo_health = do_everything(master_sample, 'PMS_prox_idx_health', 2:6)
```


### Childcare 

```{r}
res_algo_childcare = do_everything(master_sample, 'PMS_prox_idx_childcare', 2:6)
```


### Parks 

```{r}
res_algo_parks = do_everything(master_sample, 'PMS_prox_idx_parks', 2:6)
```

### Education Primary 

```{r}
res_algo_educpri = do_everything(master_sample, 'PMS_prox_idx_educpri', 2:6)
```

### Transit 

```{r}
res_algo_transit = do_everything(master_sample, 'PMS_prox_idx_transit', 2:6)
```

### Pharma 

```{r}
res_algo_pharma = do_everything(master_sample, 'PMS_prox_idx_pharma', 2:6)
```

### Education Secondary 

```{r}
res_algo_educsec = do_everything(master_sample, 'PMS_prox_idx_educsec', 2:6)
```

### Grocery 

```{r}
res_algo_grocery = do_everything(master_sample, 'PMS_prox_idx_grocery', 2:6)
```

### Library 

```{r}
res_algo_lib = do_everything(master_sample, 'PMS_prox_idx_lib', 2:6)
```


<!--

## Test/Play with clustering package

```{r}
emp <- as.matrix(na.omit(master_sample[, c("PMS_prox_idx_emp")]))
```

```{r}
emp <- log(emp+0.0001)
```

```{r}
emp <- emp[is.finite(emp)]
```


```{r}
r <- algo(as.matrix(emp), 2:6)
```

```{r}
r
```

```{r}
# direct applying
res = VarSelCluster(emp, gvals=2:6, nbcores = 4, vbleSelec = FALSE)
```


```{r}
# Obtain the cluster assignments from VarSelCluster
cluster_assignments <- res@partitions@zMAP

# Obtain the data from VarselCluster
cluster_data <- res@data@dataContinuous@data

xie_coefs = intCriteria(as.matrix(cluster_data), 
                                     as.integer(cluster_assignments), 
                                     'Xie_Beni')
```


```{r}
VarSelShiny(res)
```

```{r}
# Estimated partition
fitted(res)
```

```{r}
# Estimated probabilities of classification
head(fitted(res, type="probability"))
```

```{r}
# Summary of the best model
summary(res)
```

```{r}
# the structure of s4 object 
# str(cluster_df@data)
```


```{r eval=FALSE}

# Obtain the cluster assignments from VarSelCluster
cluster_assignments <- res@partitions@zMAP

# Obtain the data from VarselCluster
cluster_data <- res@data@dataContinuous@data

# Calculate the silhouette scores
silhouette_scores <- intCriteria(as.matrix(cluster_data), as.integer(cluster_assignments), 'Silhouette') 

```

```{r}
silhouette_scores
```


```{r eval=FALSE}
# silhouette_scores_list <- c()
silhouette_scores_list[4] <- c(silhouette_scores)
```

-->



















